{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/yash/Desktop/yash-mtp/src/common\")\n",
    "from Model import *\n",
    "import os\n",
    "from SilenceRemover import *\n",
    "from datasets import Dataset\n",
    "from multiprocess import set_start_method\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import  AutoConfig, Wav2Vec2Processor\n",
    "import librosa\n",
    "from torch import mps\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "def whatDevice():\n",
    "    if  torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    return \"cpu\"\n",
    "device = whatDevice()\n",
    "print(f\"Device: {device}\")\n",
    "mps.empty_cache()\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"/Users/yash/Desktop/MTP-2k23-24/Bhashini_Test_Data\"\n",
    "### Intializing models\n",
    "## for wave2vec2\n",
    "model_name_or_path = \"yashcode00/wav2vec2-large-xlsr-indian-language-classification-featureExtractor\"\n",
    "config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name_or_path)\n",
    "model_wave2vec2 = Wav2Vec2ForSpeechClassification.from_pretrained(model_name_or_path).to(device)\n",
    "target_sampling_rate = processor.feature_extractor.sampling_rate\n",
    "processor.feature_extractor.return_attention_mask = True\n",
    "label_list  = ['asm', 'ben', 'eng', 'guj', 'hin', 'kan', 'mal', 'mar', 'odi', 'tam', 'tel']\n",
    "lang2id = {'asm': 0, 'ben': 1, 'eng': 2, 'guj': 3, 'hin': 4, 'kan': 5, 'mal': 6, 'mar': 7, 'odi': 8, 'tam': 9, 'tel': 10,'pun': 10}\n",
    "id2lang = {0: 'asm', 1: 'ben', 2: 'eng', 3: 'guj', 4: 'hin', 5: 'kan', 6: 'mal', 7: 'mar', 8: 'odi', 9: 'tam', 10: 'tel'}\n",
    "input_column = 'path'\n",
    "output_column = 'true_label'\n",
    "window_size = 16000\n",
    "hop_length_seconds = 1\n",
    "# Calculate the hop size in samples\n",
    "hop_size = int(hop_length_seconds * target_sampling_rate)  # Adjust 'sample_rate' as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractLabel(name: str):\n",
    "    return name.split(\"_\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Loading the data\n",
    "df_test = {input_column:[],output_column:[]}\n",
    "for audios in os.listdir(directory):\n",
    "    if not audios.startswith(\".\"):\n",
    "        df_test[\"path\"].append(os.path.join(directory,audios))\n",
    "        df_test[\"true_label\"].append(extractLabel(audios))\n",
    "\n",
    "## Convert this dict into huggingface datset for ease\n",
    "df_test = Dataset.from_dict(df_test)\n",
    "print(f\"The Test Data looks like: \\n{df_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_file_to_array_fn(path: str):\n",
    "    speech_array, sampling_rate = librosa.load(path, sr=target_sampling_rate)\n",
    "    # speech_array = RemoveSilenceFromArray(speech_array, target_sampling_rate)\n",
    "    return speech_array\n",
    "\n",
    "def label_to_id(label, label_list):\n",
    "    if len(label_list) > 0:\n",
    "        return label_list.index(label) if label in label_list else -1\n",
    "    return label\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    speech_list = [np.array(speech_file_to_array_fn(path),dtype=np.float32) for path in examples[input_column]]\n",
    "    examples['speech'] = list(speech_list)\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df_test.map(\n",
    "    preprocess_function,\n",
    "    batched=True, \n",
    "    batch_size=16\n",
    ")\n",
    "print(f\"After preprocessing: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to store the hidden feature representation from the last layer of wave2vec2\n",
    "def predictOneSecond(frames):\n",
    "    features = processor(frames, sampling_rate=processor.feature_extractor.sampling_rate, return_tensors=\"pt\", padding=True)\n",
    "    input_values = features.input_values.to(device)\n",
    "    attention_mask = features.attention_mask.to(device)\n",
    "    # print(f\"shape of the processed input is: {input_values.shape}\")\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            logits = model_wave2vec2(input_values, attention_mask=attention_mask).logits \n",
    "    except Exception as err:\n",
    "        print(f\"Error -> {err} \\nSKIPPED! Input Length was: {len(frames[-1])} and features len was : {input_values.shape}\")\n",
    "    return logits\n",
    "\n",
    "\n",
    "def predictOne(x):\n",
    "    # Generate overlapping frames\n",
    "    frames = [x[i:i+window_size] for i in range(0, len(x) - window_size + 1, hop_size)]\n",
    "    # print(frames)\n",
    "    # print(\"len of the audio splitted into one second chunks: \",len(frames))\n",
    "    if len(frames[-1])<100:\n",
    "        print(f\"Last element has small length of {len(frames[-1])} while it shall be {len(frames[0])}, Dropping!\")\n",
    "        frames.pop()\n",
    "    logits = predictOneSecond(frames)\n",
    "    preds = torch.argmax(logits, dim=-1).detach().cpu().numpy()\n",
    "    preds = np.argmax(np.bincount(preds))\n",
    "    return preds\n",
    "\n",
    "def predict(batches):\n",
    "    preds = [predictOne(arr) for arr in batches['speech']]\n",
    "    batches['predicted'] = preds\n",
    "    return batches\n",
    "\n",
    "result2 = result.map(\n",
    "    predict,\n",
    "    batched=True,\n",
    "    batch_size=16,\n",
    "    # num_proc=4,\n",
    ")\n",
    "print(f\"After predictions: {result2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [lang2id[name] for name in result[output_column]]\n",
    "y_pred = result2[\"predicted\"]\n",
    "\n",
    "print(y_true[:15])\n",
    "print(y_pred[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = classification_report(y_true, y_pred, target_names=label_list)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  precision    recall  f1-score   support\n",
    "\n",
    "         asm       0.80      1.00      0.89         4\n",
    "         ben       1.00      0.20      0.33         5\n",
    "         eng       0.00      0.00      0.00         4\n",
    "         guj       0.80      1.00      0.89         4\n",
    "         hin       0.25      0.50      0.33         4\n",
    "         kan       0.50      1.00      0.67         4\n",
    "         mal       1.00      0.60      0.75         5\n",
    "         mar       0.43      0.60      0.50         5\n",
    "         odi       1.00      0.60      0.75         5\n",
    "         tam       0.71      1.00      0.83         5\n",
    "         tel       0.50      0.38      0.43         8\n",
    "\n",
    "    accuracy                           0.60        53\n",
    "   macro avg       0.64      0.62      0.58        53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/yash/Desktop/MTP-2k23-24/TTS_data_SilenceRemovedData/hin/train_hindifullfemale_04219.wav\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
